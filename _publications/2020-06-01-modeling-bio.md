---
title: "Modeling Biological Immunity to Adversarial Examples"
collection: publications
permalink: /publication/2020-06-01-modeling-bio
excerpt: 'In a general sense, adversarial attack through perturbations is not a machine learning vulnerability. Human and
biological vision can also be fooled by various methods, i.e.
mixing high and low frequency images together, by altering semantically related signals, or by sufficiently distorting the input signal. However, the amount and magnitude
of such a distortion required to alter biological perception
is at a much larger scale. In this work, we explored this gap
through the lens of biology and neuroscience in order to understand the robustness exhibited in human perception. Our
experiments show that by leveraging sparsity and modeling
the biological mechanisms at a cellular level, we are able
to mitigate the effect of adversarial alterations to the signal
that have no perceptible meaning. Furthermore, we present
and illustrate the effects of top-down functional processes
that contribute to the inherent immunity in human perception in the context of exploiting these properties to make a
more robust machine vision system.'
date: 2020-06-01
venue: 'IEEE Computer Vision and Pattern Recognition, CVPR'
paperurl: 'http://edk208.github.io/files/07828.pdf'
---
This paper is about the number 3. The number 4 is left for future work.

[Download paper here](http://edk208.github.io/files/07828.pdf)

